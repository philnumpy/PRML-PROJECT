{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMryKWpf6q9BrU8AjcpUMnp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philnumpy/PRML-PROJECT/blob/main/KMeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>KMEANS"
      ],
      "metadata": {
        "id": "Le9tn8BIAwZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K means** is an ***unsupervised machine learning*** technique that helps us group *unlabeled* data into various clusters on the basis of their similarity.\n",
        "\n",
        "It is a ***“hard”*** clustering method. This form of grouping stipulates that a data point can exist in just one cluster.\n",
        "\n",
        "It is an ***iterative, centroid-based*** clustering algorithm that partitions a dataset into similar groups based on the distance between their centroids. The centroid, or cluster center, is either the mean or median of all the points within the cluster depending on the characteristics of the data."
      ],
      "metadata": {
        "id": "MacfO44WA0NW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uPrWYxQvFJTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h3>Importing all important libraries**"
      ],
      "metadata": {
        "id": "BIcGBlGHHGny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd #panel data that helps manipulate data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split #to train test and split just in case\n",
        "from sklearn.decomposition import PCA #Principal Component Analysis\n",
        "from sklearn.preprocessing import StandardScaler #to scale and normalise data"
      ],
      "metadata": {
        "id": "b_QGxuCVHFz_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Data**"
      ],
      "metadata": {
        "id": "ts_5b1OqLzPC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZKTLgCALyzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling and Normalising data**"
      ],
      "metadata": {
        "id": "dafIaba1IHyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler=StandardScaler()\n",
        "data= scaler.fit_tranform(data) #to centre and scale the data preparing it for PCA s.t. mean is zero and standard deviation is 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "P43GctuJIHIn",
        "outputId": "4d71f5a6-7694-450d-ebec-8e1706e67f6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'StandardScaler' object has no attribute 'fit_tranform'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e3285f1d99ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_tranform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#to centre and scale the data preparing it for PCA s.t. mean is zero and standard deviation is 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'StandardScaler' object has no attribute 'fit_tranform'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<H3>PCA**"
      ],
      "metadata": {
        "id": "Shn1Xu4zOFtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In SKLEARN,\n",
        "Variation is calculated as:\n",
        "\n",
        "(measurement-mean)^2/no. of samples\n",
        "\n",
        "Below is the part where we do all the meansurements"
      ],
      "metadata": {
        "id": "RPEv2nvpRfx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)  #reducing the number of components to 2 since it is easier to observe in terms of clusters\n",
        "data_pca = pca.fit_transform(data_scaled) #where we do all the pca math of loading scores and variations each pca accounts for(basically co-ordinates)\n",
        "#we generate scores for a pca graph and variations"
      ],
      "metadata": {
        "id": "qtY7G8NoIGur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h3>Optimising K means</h3>**\n",
        "\n",
        "1. **Properly selecting the initial centroids**\n",
        "\n",
        " K Means++ is a more advanced method to initialise centroids as starting them out as random is a always a bad choice.\n"
      ],
      "metadata": {
        "id": "Ya4vW0KgFfbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#K-Means++ for initial centroids\n",
        "def kmeans_plus_plus(X, k): #gets data and k's value\n",
        "    \"\"\"Initialize centroids using K-Means++ method.\"\"\"\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    #Randomly selecting the first centroid\n",
        "    centroids = [X[np.random.randint(n_samples)]]\n",
        "\n",
        "    for i in range(1, k):\n",
        "        #Computing squared distances to the nearest centroid\n",
        "        distances = np.array([min(np.linalg.norm(x - c) ** 2 for c in centroids) for x in X])\n",
        "\n",
        "        #Choosing next centroid on the basis of probability proportional to distance²\n",
        "        probabilities = distances / distances.sum()\n",
        "        new_centroid_index = np.random.choice(n_samples, p=probabilities)\n",
        "\n",
        "        # Adding the new centroid\n",
        "        centroids.append(X[new_centroid_index])\n",
        "\n",
        "    return np.array(centroids)"
      ],
      "metadata": {
        "id": "SYqYiBtPAv9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize centroids using K-Means++\n",
        "initial_centroids = kmeans_plus_plus(X, k)\n",
        "print(\"Initial Centroids:\\n\", initial_centroids)"
      ],
      "metadata": {
        "id": "Y2Off61oV_6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFKx0mVCAY_e"
      },
      "outputs": [],
      "source": [
        "class KMeans:\n",
        "    def __init__(self, k=None, max_k=10, max_iters=100, tol=1e-4, auto_k=True):\n",
        "        \"\"\"\n",
        "        k: Number of clusters (if None, auto-detect using Elbow method)\n",
        "        max_k: Max clusters for Elbow method (used if auto_k=True)\n",
        "        max_iters: Maximum iterations for convergence\n",
        "        tol: Tolerance to stop iterations early\n",
        "        auto_k: If True, automatically find the best k using Elbow method\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "        self.auto_k = auto_k\n",
        "        self.max_k = max_k\n",
        "\n",
        "    def fit(self, X):\n",
        "\n",
        "        if self.auto_k and self.k is None:\n",
        "            self.k = self.find_optimal_k(X)  # Find best k before clustering\n",
        "        # Using K-Means++ for better initialisation\n",
        "        self.centroids = kmeans_plus_plus(X, self.k)\n",
        "\n",
        "        for i in range(self.max_iters):\n",
        "            #Assigning points to nearest centroid, this is where clusters are forming\n",
        "            #np.argmin returns the index of the closest centroid\n",
        "            self.labels = np.array([np.argmin([np.linalg.norm(x - c) for c in self.centroids]) for x in X]) #euclidean distance is being considered\n",
        "\n",
        "            #Computing new centroids\n",
        "            '''\n",
        "            For each cluster j, calculating the mean of all points assigned to it.\n",
        "\n",
        "            This updates the centroid locations based on the new cluster assignments.\n",
        "            for eg.\n",
        "            cluster 1: (1,2) and (2,4) new mean will be (1.5,3) New Centroid-1\n",
        "            cluster 2: (0,2) and (1,1.5) new mean will be (0.5,1.75) New Centroid-2\n",
        "            '''\n",
        "            new_centroids = np.array([X[self.labels == j].mean(axis=0) for j in range(self.k)])\n",
        "\n",
        "            #Checking for convergence\n",
        "            '''\n",
        "            If centroids don’t change much (tol = small threshold),we stop iterating.\n",
        "\n",
        "            This ensures that the algorithm stops when clusters are stable.\n",
        "            '''\n",
        "            if np.all(np.abs(new_centroids - self.centroids) < self.tol):\n",
        "                break\n",
        "\n",
        "            self.centroids = new_centroids  # Update centroids and the loop continues\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([np.argmin([np.linalg.norm(x - c) for c in self.centroids]) for x in X])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Optimum number of clusters improves the accuracy**\n",
        "\n",
        " It measures the euclidean distance between each data point and its cluster center and chooses the number of clusters based on where change in “within cluster sum of squares” (WCSS) levels off. This value represents the total variance within each cluster that gets plotted against the number of clusters. ***Best K is where the curve forms an elbow (sudden drop, then flattening).***"
      ],
      "metadata": {
        "id": "3phEzZt-YQMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def find_optimal_k(self, X):\n",
        "        \"\"\" Automatically find the best k using the Elbow Method & Knee Point \"\"\"\n",
        "        wcss = []\n",
        "        for k in range(1, self.max_k + 1):\n",
        "            kmeans = KMeansFromScratch(k=k, auto_k=False)  # Run K-Means manually\n",
        "            kmeans.fit(X)\n",
        "            wcss.append(sum(np.linalg.norm(X[kmeans.labels == j] - kmeans.centroids[j]) ** 2 for j in range(k)))\n",
        "\n",
        "        return self.knee_locator(range(1, self.max_k + 1), wcss)\n",
        "\n",
        "    @staticmethod\n",
        "    def knee_locator(x, y):\n",
        "        \"\"\" Find the 'knee' point (elbow) in the WCSS graph \"\"\"\n",
        "        x, y = np.array(x), np.array(y)\n",
        "        # Compute second derivative (curvature)\n",
        "        diff = np.gradient(y, edge_order=2)\n",
        "        elbow_index = np.argmin(diff) + 1  # Adding 1 to match k index\n",
        "        return x[elbow_index]\n",
        "\n",
        "    def plot_elbow(self, X):\n",
        "        \"\"\" Plot WCSS vs. k to visualize the Elbow Method \"\"\"\n",
        "        wcss = []\n",
        "        for k in range(1, self.max_k + 1):\n",
        "            kmeans = KMeans(k=k, auto_k=False)\n",
        "            kmeans.fit(X)\n",
        "            wcss.append(sum(np.linalg.norm(X[kmeans.labels == j] - kmeans.centroids[j]) ** 2 for j in range(k)))\n",
        "\n",
        "        best_k = self.knee_locator(range(1, self.max_k + 1), wcss)\n",
        "\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(range(1, self.max_k + 1), wcss, marker='o', linestyle='--', color='b')\n",
        "        plt.xlabel('Number of Clusters (k)')\n",
        "        plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "        plt.title('Elbow Method for Optimal K')\n",
        "        plt.axvline(x=best_k, linestyle='--', color='r', label=f'Optimal K = {best_k}')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        print(f\"Optimal K found: {best_k}\")"
      ],
      "metadata": {
        "id": "iUWfaUDTYRwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hDgUsaiLSyZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **<h1> Cluster Evaluation Metrics </h1>**\n",
        "\n",
        "1. All data points within a cluster should be similar.\n",
        "\n",
        "2. Clusters should be distinct from each other.\n",
        "\n",
        "The goal of the k-means clustering algorithm is to minimize the sum of squared errors (SSE)"
      ],
      "metadata": {
        "id": "UMqNuFRta4CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h3>Inertia</h3>**\n",
        "\n",
        "Inertia is calculated by measuring the distance between a datapoint and its centroid, squaring the distance and summing those squares for each data point in the cluster. The sum or inertial value is the intracluster distance. The lower the sum the better because it means that the datapoints within the cluster are compact or more similar."
      ],
      "metadata": {
        "id": "E1aN711Va6LK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IV4eGalYbEFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<h3>The Dunn index</h3>**\n",
        "\n",
        "The second property is measured with the Dunn index. The Dunn index represents the relationship between the minimum intercluster distance and the maximum intracluster distance. Clusters with a high intercluster distance indicate better quality because it means that the clusters are as different from each other as possible."
      ],
      "metadata": {
        "id": "lSePdDZPbCoL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KB-B69M-bDpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5cDjIFXva4Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Sources:\n",
        "\n",
        "\n",
        "1.   https://www.ibm.com/think/topics/k-means-clustering\n",
        "2.   List item\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "A1NRobNuErDY"
      }
    }
  ]
}